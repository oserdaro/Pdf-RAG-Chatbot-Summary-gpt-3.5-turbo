{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# WELCOME\n",
        "\n",
        "This notebook will guide you through two increasingly significant applications in the realm of Generative AI: RAG (Retrieval Augmented Generation) chatbots and text summarization for big text."
      ],
      "metadata": {
        "id": "DxOaSxtJWV1G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Project 1: Building a Chatbot with a PDF Document (RAG)\n",
        "\n",
        "In this project, we will develop a chatbot using a provided PDF document from web page. We will utilize the Langchain framework along with a large language model (LLM) such as GPT or Gemini. The chatbot will leverage the Retrieval Augmented Generation (RAG) technique to comprehend the document's content and respond to user queries effectively.\n",
        "\n",
        "### **Project Steps:**\n",
        "\n",
        "- **1.PDF Document Upload:** Upload the provided PDF document from web page (https://aclanthology.org/N19-1423.pdf) (BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding).\n",
        "\n",
        "- **2.Chunking:** Divide the uploaded PDF document into smaller segments (chunks). This facilitates more efficient information processing by the LLM.\n",
        "\n",
        "- **3.ChromaDB Setup:**\n",
        "  - Save ChromaDB to your Google Drive.\n",
        "\n",
        "  - Retrieve ChromaDB from your Drive to begin using it in your project.\n",
        "\n",
        "  - ChromaDB serves as a vector database to store embedding vectors generated from your document.\n",
        "\n",
        "- **4.Embedding Vectors Creation:**\n",
        "  - Convert the chunked document into embedding vectors. Use either GPT or Gemini embedding models for this purpose.\n",
        "\n",
        "  - If you choose the Gemini embedding model, set \"task_type\" to \"retrieval_document\" when converting the chunked document.\n",
        "\n",
        "- **5.Chatbot Development:**\n",
        "  - Utilize the **load_qa_chain** function from the Langchain library to build the chatbot.\n",
        "\n",
        "  - This function will interpret user queries, retrieve relevant information from **ChromaDB**, and generate responses accordingly.\n",
        "\n"
      ],
      "metadata": {
        "id": "MaCz7nhxKI9R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install Libraries"
      ],
      "metadata": {
        "id": "_eoQWi-uN0dx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain"
      ],
      "metadata": {
        "id": "PCbI4MuNanVu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "849912e9-ebd9-4215-cc7b-868a7cd5a7df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.1.20-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.30)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.6.6-py3-none-any.whl (28 kB)\n",
            "Collecting langchain-community<0.1,>=0.0.38 (from langchain)\n",
            "  Downloading langchain_community-0.0.38-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-core<0.2.0,>=0.1.52 (from langchain)\n",
            "  Downloading langchain_core-0.1.52-py3-none-any.whl (302 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.9/302.9 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-text-splitters<0.1,>=0.0.1 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.0.1-py3-none-any.whl (21 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
            "  Downloading langsmith-0.1.57-py3-none-any.whl (121 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.0/121.0 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.7.1)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.3.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.21.2-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.2.0,>=0.1.52->langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting packaging<24.0,>=23.2 (from langchain-core<0.2.0,>=0.1.52->langchain)\n",
            "  Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading orjson-3.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.18.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.2.0,>=0.1.52->langchain)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: packaging, orjson, mypy-extensions, jsonpointer, typing-inspect, marshmallow, jsonpatch, langsmith, dataclasses-json, langchain-core, langchain-text-splitters, langchain-community, langchain\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.0\n",
            "    Uninstalling packaging-24.0:\n",
            "      Successfully uninstalled packaging-24.0\n",
            "Successfully installed dataclasses-json-0.6.6 jsonpatch-1.33 jsonpointer-2.4 langchain-0.1.20 langchain-community-0.0.38 langchain-core-0.1.52 langchain-text-splitters-0.0.1 langsmith-0.1.57 marshmallow-3.21.2 mypy-extensions-1.0.0 orjson-3.10.3 packaging-23.2 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain-openai"
      ],
      "metadata": {
        "id": "qOaahY-AancA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "008536c6-67aa-4ca3-f433-8d49e4711745"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.6/320.6 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chromadb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZGCKcXw1LZ59",
        "outputId": "be49fc9a-e7e6-4934-f010-469002286a8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting chromadb\n",
            "  Downloading chromadb-0.5.0-py3-none-any.whl (526 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m526.8/526.8 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.2.1)\n",
            "Requirement already satisfied: requests>=2.28 in /usr/local/lib/python3.10/dist-packages (from chromadb) (2.31.0)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.10/dist-packages (from chromadb) (2.7.1)\n",
            "Collecting chroma-hnswlib==0.7.3 (from chromadb)\n",
            "  Downloading chroma_hnswlib-0.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastapi>=0.95.2 (from chromadb)\n",
            "  Downloading fastapi-0.111.0-py3-none-any.whl (91 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uvicorn[standard]>=0.18.3 (from chromadb)\n",
            "  Downloading uvicorn-0.29.0-py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.25.2)\n",
            "Collecting posthog>=2.4.0 (from chromadb)\n",
            "  Downloading posthog-3.5.0-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.11.0)\n",
            "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
            "  Downloading onnxruntime-1.17.3-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opentelemetry-api>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_api-1.24.0-py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.1/60.1 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.24.0-py3-none-any.whl (18 kB)\n",
            "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n",
            "  Downloading opentelemetry_instrumentation_fastapi-0.45b0-py3-none-any.whl (11 kB)\n",
            "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_sdk-1.24.0-py3-none-any.whl (106 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.1/106.1 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.19.1)\n",
            "Collecting pypika>=0.48.9 (from chromadb)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.66.4)\n",
            "Collecting overrides>=7.3.1 (from chromadb)\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.4.0)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.63.0)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb)\n",
            "  Downloading bcrypt-4.1.3-cp39-abi3-manylinux_2_28_x86_64.whl (283 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m283.7/283.7 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.9.4)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb)\n",
            "  Downloading kubernetes-29.0.0-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (8.3.0)\n",
            "Requirement already satisfied: PyYAML>=6.0.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.0.1)\n",
            "Collecting mmh3>=4.0.1 (from chromadb)\n",
            "  Downloading mmh3-4.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.10/dist-packages (from chromadb) (3.10.3)\n",
            "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (23.2)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (1.1.0)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (2.0.1)\n",
            "Collecting starlette<0.38.0,>=0.37.2 (from fastapi>=0.95.2->chromadb)\n",
            "  Downloading starlette-0.37.2-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastapi-cli>=0.0.2 (from fastapi>=0.95.2->chromadb)\n",
            "  Downloading fastapi_cli-0.0.3-py3-none-any.whl (9.2 kB)\n",
            "Requirement already satisfied: httpx>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from fastapi>=0.95.2->chromadb) (0.27.0)\n",
            "Requirement already satisfied: jinja2>=2.11.2 in /usr/local/lib/python3.10/dist-packages (from fastapi>=0.95.2->chromadb) (3.1.4)\n",
            "Collecting python-multipart>=0.0.7 (from fastapi>=0.95.2->chromadb)\n",
            "  Downloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
            "Collecting ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 (from fastapi>=0.95.2->chromadb)\n",
            "  Downloading ujson-5.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting email_validator>=2.0.0 (from fastapi>=0.95.2->chromadb)\n",
            "  Downloading email_validator-2.1.1-py3-none-any.whl (30 kB)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2024.2.2)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.27.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.3.1)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.7)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (24.3.25)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.12)\n",
            "Collecting deprecated>=1.2.6 (from opentelemetry-api>=1.2.0->chromadb)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting importlib-metadata<=7.0,>=6.0 (from opentelemetry-api>=1.2.0->chromadb)\n",
            "  Downloading importlib_metadata-7.0.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.63.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.24.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.24.0-py3-none-any.whl (17 kB)\n",
            "Collecting opentelemetry-proto==1.24.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_proto-1.24.0-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opentelemetry-instrumentation-asgi==0.45b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_instrumentation_asgi-0.45b0-py3-none-any.whl (14 kB)\n",
            "Collecting opentelemetry-instrumentation==0.45b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_instrumentation-0.45b0-py3-none-any.whl (28 kB)\n",
            "Collecting opentelemetry-semantic-conventions==0.45b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_semantic_conventions-0.45b0-py3-none-any.whl (36 kB)\n",
            "Collecting opentelemetry-util-http==0.45b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_util_http-0.45b0-py3-none-any.whl (6.9 kB)\n",
            "Requirement already satisfied: setuptools>=16.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.45b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (67.7.2)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.45b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.14.1)\n",
            "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.45b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
            "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9->chromadb) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9->chromadb) (2.18.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->chromadb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->chromadb) (3.7)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers>=0.13.2->chromadb) (0.20.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.14.0)\n",
            "Collecting httptools>=0.5.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (341 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dotenv>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading watchfiles-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m43.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dnspython>=2.0.0 (from email_validator>=2.0.0->fastapi>=0.95.2->chromadb)\n",
            "  Downloading dnspython-2.6.1-py3-none-any.whl (307 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typer>=0.9.0 (from chromadb)\n",
            "  Downloading typer-0.12.3-py3-none-any.whl (47 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.2/47.2 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting shellingham>=1.3.0 (from typer>=0.9.0->chromadb)\n",
            "  Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (13.7.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.23.0->fastapi>=0.95.2->chromadb) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.23.0->fastapi>=0.95.2->chromadb) (1.0.5)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.23.0->fastapi>=0.95.2->chromadb) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.14.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2023.6.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<=7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.18.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.11.2->fastapi>=0.95.2->chromadb) (2.1.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer>=0.9.0->chromadb) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer>=0.9.0->chromadb) (2.16.1)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.23.0->fastapi>=0.95.2->chromadb) (1.2.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.9.0->chromadb) (0.1.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.0)\n",
            "Building wheels for collected packages: pypika\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53724 sha256=39f6701e19fb97b9fe727deee1496495f23b0a68a3355edf7948f62b22238d7e\n",
            "  Stored in directory: /root/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\n",
            "Successfully built pypika\n",
            "Installing collected packages: pypika, monotonic, mmh3, websockets, uvloop, uvicorn, ujson, shellingham, python-multipart, python-dotenv, overrides, opentelemetry-util-http, opentelemetry-semantic-conventions, opentelemetry-proto, importlib-metadata, humanfriendly, httptools, dnspython, deprecated, chroma-hnswlib, bcrypt, backoff, asgiref, watchfiles, starlette, posthog, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, email_validator, coloredlogs, typer, opentelemetry-sdk, opentelemetry-instrumentation, onnxruntime, kubernetes, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, opentelemetry-instrumentation-fastapi, fastapi-cli, fastapi, chromadb\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib_metadata 7.1.0\n",
            "    Uninstalling importlib_metadata-7.1.0:\n",
            "      Successfully uninstalled importlib_metadata-7.1.0\n",
            "  Attempting uninstall: typer\n",
            "    Found existing installation: typer 0.9.4\n",
            "    Uninstalling typer-0.9.4:\n",
            "      Successfully uninstalled typer-0.9.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "spacy 3.7.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\n",
            "weasel 0.3.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed asgiref-3.8.1 backoff-2.2.1 bcrypt-4.1.3 chroma-hnswlib-0.7.3 chromadb-0.5.0 coloredlogs-15.0.1 deprecated-1.2.14 dnspython-2.6.1 email_validator-2.1.1 fastapi-0.111.0 fastapi-cli-0.0.3 httptools-0.6.1 humanfriendly-10.0 importlib-metadata-7.0.0 kubernetes-29.0.0 mmh3-4.1.0 monotonic-1.6 onnxruntime-1.17.3 opentelemetry-api-1.24.0 opentelemetry-exporter-otlp-proto-common-1.24.0 opentelemetry-exporter-otlp-proto-grpc-1.24.0 opentelemetry-instrumentation-0.45b0 opentelemetry-instrumentation-asgi-0.45b0 opentelemetry-instrumentation-fastapi-0.45b0 opentelemetry-proto-1.24.0 opentelemetry-sdk-1.24.0 opentelemetry-semantic-conventions-0.45b0 opentelemetry-util-http-0.45b0 overrides-7.7.0 posthog-3.5.0 pypika-0.48.9 python-dotenv-1.0.1 python-multipart-0.0.9 shellingham-1.5.4 starlette-0.37.2 typer-0.12.3 ujson-5.10.0 uvicorn-0.29.0 uvloop-0.19.0 watchfiles-0.21.0 websockets-12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pypdfium2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FWkSvjfnLgb5",
        "outputId": "83f6a0b2-7450-4fed-a4e7-8b8399bc6ad8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pypdfium2\n",
            "  Downloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdfium2\n",
            "Successfully installed pypdfium2-4.30.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Access Google Drive"
      ],
      "metadata": {
        "id": "FuLllnCl2yfe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "uQR06EhDapPP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71f5018c-597f-4aef-c25c-e011c53b47c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Entering Your OpenAI or Google Gemini API Key."
      ],
      "metadata": {
        "id": "0uR9bJp_0MyF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ['OPENAI_API_KEY']=userdata.get('openai_key')"
      ],
      "metadata": {
        "id": "2jwo1SQ2asnZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(\n",
        "  api_key=os.environ['OPENAI_API_KEY']\n",
        ")"
      ],
      "metadata": {
        "id": "90rF1aM1astv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading PDF Document"
      ],
      "metadata": {
        "id": "OV9rG-0PN8p0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a pdf reader function\n",
        "from langchain.document_loaders import PyPDFium2Loader\n",
        "\n",
        "def read_doc(directory):\n",
        "    file_loader=PyPDFium2Loader(directory)\n",
        "    pdf_documents=file_loader.load() # PyPDFium2Loader reads page by page\n",
        "    return pdf_documents"
      ],
      "metadata": {
        "id": "5H6eQyYyauxP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pdf=read_doc('/content/drive/MyDrive/FINAL PROJJECTS/N19-1423.pdf')\n",
        "len(pdf)\n",
        "\n",
        "# The document consists of 16 pages"
      ],
      "metadata": {
        "id": "n_kXJZ5Taupv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94fc7b60-d537-454e-b5a3-47bcbe4d69f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pypdfium2/_helpers/textpage.py:80: UserWarning: get_text_range() call with default params will be implicitly redirected to get_text_bounded()\n",
            "  warnings.warn(\"get_text_range() call with default params will be implicitly redirected to get_text_bounded()\")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# content of the 2nd page of the pdf\n",
        "pdf[2].page_content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "id": "WSHe5Y9AW4qU",
        "outputId": "1ac12439-2710-4414-886a-090d30b3dd6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'4173\\r\\nBERT BERT\\r\\nE[CLS] E1\\r\\n E[SEP] ... EN\\r\\nE1’ ... EM’\\r\\nC T1 T[SEP] ... TN\\r\\nT1’ ... TM’\\r\\n[CLS] Tok 1 [SEP] ... Tok N Tok 1 ... TokM\\r\\nQuestion Paragraph\\r\\nStart/End Span\\r\\nBERT\\r\\nE[CLS] E1\\r\\n E[SEP] ... EN\\r\\nE1’ ... EM’\\r\\nC T1 T[SEP] ... TN\\r\\nT1’ ... TM’\\r\\n[CLS] Tok 1 [SEP] ... Tok N Tok 1 ... TokM\\r\\nMasked Sentence A Masked Sentence B\\r\\nPre-training Fine-Tuning\\r\\nNSP Mask LM Mask LM\\r\\nUnlabeled Sentence A and B Pair \\r\\nSQuAD\\r\\nQuestion Answer Pair\\r\\nMNLI NER\\r\\nFigure 1: Overall pre-training and fine-tuning procedures for BERT. Apart from output layers, the same architec\\x02tures are used in both pre-training and fine-tuning. The same pre-trained model parameters are used to initialize\\r\\nmodels for different down-stream tasks. During fine-tuning, all parameters are fine-tuned. [CLS] is a special\\r\\nsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques\\x02tions/answers).\\r\\ning and auto-encoder objectives have been used\\r\\nfor pre-training such models (Howard and Ruder,\\r\\n2018; Radford et al., 2018; Dai and Le, 2015).\\r\\n2.3 Transfer Learning from Supervised Data\\r\\nThere has also been work showing effective trans\\x02fer from supervised tasks with large datasets, such\\r\\nas natural language inference (Conneau et al.,\\r\\n2017) and machine translation (McCann et al.,\\r\\n2017). Computer vision research has also demon\\x02strated the importance of transfer learning from\\r\\nlarge pre-trained models, where an effective recipe\\r\\nis to fine-tune models pre-trained with Ima\\x02geNet (Deng et al., 2009; Yosinski et al., 2014).\\r\\n3 BERT\\r\\nWe introduce BERT and its detailed implementa\\x02tion in this section. There are two steps in our\\r\\nframework: pre-training and fine-tuning. Dur\\x02ing pre-training, the model is trained on unlabeled\\r\\ndata over different pre-training tasks. For fine\\x02tuning, the BERT model is first initialized with\\r\\nthe pre-trained parameters, and all of the param\\x02eters are fine-tuned using labeled data from the\\r\\ndownstream tasks. Each downstream task has sep\\x02arate fine-tuned models, even though they are ini\\x02tialized with the same pre-trained parameters. The\\r\\nquestion-answering example in Figure 1 will serve\\r\\nas a running example for this section.\\r\\nA distinctive feature of BERT is its unified ar\\x02chitecture across different tasks. There is mini\\x02mal difference between the pre-trained architec\\x02ture and the final downstream architecture.\\r\\nModel Architecture BERT’s model architec\\x02ture is a multi-layer bidirectional Transformer en\\x02coder based on the original implementation de\\x02scribed in Vaswani et al. (2017) and released in\\r\\nthe tensor2tensor library.1 Because the use\\r\\nof Transformers has become common and our im\\x02plementation is almost identical to the original,\\r\\nwe will omit an exhaustive background descrip\\x02tion of the model architecture and refer readers to\\r\\nVaswani et al. (2017) as well as excellent guides\\r\\nsuch as “The Annotated Transformer.”2\\r\\nIn this work, we denote the number of layers\\r\\n(i.e., Transformer blocks) as L, the hidden size as\\r\\nH, and the number of self-attention heads as A.\\r\\n3\\r\\nWe primarily report results on two model sizes:\\r\\nBERTBASE (L=12, H=768, A=12, Total Param\\x02eters=110M) and BERTLARGE (L=24, H=1024,\\r\\nA=16, Total Parameters=340M).\\r\\nBERTBASE was chosen to have the same model\\r\\nsize as OpenAI GPT for comparison purposes.\\r\\nCritically, however, the BERT Transformer uses\\r\\nbidirectional self-attention, while the GPT Trans\\x02former uses constrained self-attention where every\\r\\ntoken can only attend to context to its left.4\\r\\n1\\r\\nhttps://github.com/tensorflow/tensor2tensor\\r\\n2\\r\\nhttp://nlp.seas.harvard.edu/2018/04/03/attention.html\\r\\n3\\r\\nIn all cases we set the feed-forward/filter size to be 4H,\\r\\ni.e., 3072 for the H = 768 and 4096 for the H = 1024.\\r\\n4We note that in the literature the bidirectional Trans-\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Document Splitter"
      ],
      "metadata": {
        "id": "WLQ1j_JrOF57"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
        "\n",
        "\n",
        "def chunk_data(docs, chunk_size=800, chunk_overlap=200):\n",
        "    text_splitter=RecursiveCharacterTextSplitter(chunk_size=chunk_size,\n",
        "                                                 chunk_overlap=chunk_overlap)\n",
        "    pdf=text_splitter.split_documents(docs)\n",
        "    return pdf\n",
        "\n",
        "# This code splits documents into chunks using the RecursiveCharacterTextSplitter class from the langchain library.\n",
        "\n",
        "# A function named chunk_data is defined, which takes a document or a collection of documents (docs) as input.\n",
        "# It also takes two parameters: chunk_size and chunk_overlap.\n",
        "# chunk_size specifies the maximum number of characters in each chunk, while chunk_overlap determines the amount of overlap between consecutive chunks.\n",
        "\n",
        "# The function divides the documents into chunks based on these parameters using the RecursiveCharacterTextSplitter class.\n",
        "# Consequently, each chunk contains chunk_size characters, with an overlap of chunk_overlap characters between consecutive chunks.\n",
        "\n",
        "# As a result, the documents are segmented into chunks of specified sizes, and these chunks are returned.\n",
        "\n",
        "# The chunk_overlap parameter is used to specify the sharing of characters between consecutive chunks.\n",
        "# In other words, it ensures that the characters at the end of one chunk reappear at the beginning of the next chunk.\n",
        "# This prevents the loss of information when the text is segmented or divided and helps preserve a certain context.\n",
        "# Especially, overlap can be used to maintain important contextual relationships within a specific text and sustain meaning across chunks.\n"
      ],
      "metadata": {
        "id": "HHQlclU9awwa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_doc=chunk_data(docs=pdf)\n",
        "len(pdf_doc)\n",
        "\n",
        "# divided into 112 pieces"
      ],
      "metadata": {
        "id": "NaQV6XRwawpf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1029769c-25dd-4ba4-c189-b96cdfb0ab73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "112"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_doc[15:17]\n",
        "\n",
        "# Parts 15 and 16.\n",
        "# As you can see, the last 200 characters of the 25th piece and the first 200 characters of the 26th piece are the same."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4gpo1_bXXgFJ",
        "outputId": "9097a44e-a903-401e-ab96-7400eee94432"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='4173\\r\\nBERT BERT\\r\\nE[CLS] E1\\r\\n E[SEP] ... EN\\r\\nE1’ ... EM’\\r\\nC T1 T[SEP] ... TN\\r\\nT1’ ... TM’\\r\\n[CLS] Tok 1 [SEP] ... Tok N Tok 1 ... TokM\\r\\nQuestion Paragraph\\r\\nStart/End Span\\r\\nBERT\\r\\nE[CLS] E1\\r\\n E[SEP] ... EN\\r\\nE1’ ... EM’\\r\\nC T1 T[SEP] ... TN\\r\\nT1’ ... TM’\\r\\n[CLS] Tok 1 [SEP] ... Tok N Tok 1 ... TokM\\r\\nMasked Sentence A Masked Sentence B\\r\\nPre-training Fine-Tuning\\r\\nNSP Mask LM Mask LM\\r\\nUnlabeled Sentence A and B Pair \\r\\nSQuAD\\r\\nQuestion Answer Pair\\r\\nMNLI NER\\r\\nFigure 1: Overall pre-training and fine-tuning procedures for BERT. Apart from output layers, the same architec\\x02tures are used in both pre-training and fine-tuning. The same pre-trained model parameters are used to initialize\\r\\nmodels for different down-stream tasks. During fine-tuning, all parameters are fine-tuned. [CLS] is a special', metadata={'source': '/content/drive/MyDrive/FINAL PROJJECTS/N19-1423.pdf', 'page': 2}),\n",
              " Document(page_content='models for different down-stream tasks. During fine-tuning, all parameters are fine-tuned. [CLS] is a special\\r\\nsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques\\x02tions/answers).\\r\\ning and auto-encoder objectives have been used\\r\\nfor pre-training such models (Howard and Ruder,\\r\\n2018; Radford et al., 2018; Dai and Le, 2015).\\r\\n2.3 Transfer Learning from Supervised Data\\r\\nThere has also been work showing effective trans\\x02fer from supervised tasks with large datasets, such\\r\\nas natural language inference (Conneau et al.,\\r\\n2017) and machine translation (McCann et al.,\\r\\n2017). Computer vision research has also demon\\x02strated the importance of transfer learning from\\r\\nlarge pre-trained models, where an effective recipe', metadata={'source': '/content/drive/MyDrive/FINAL PROJJECTS/N19-1423.pdf', 'page': 2})]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Creating A Embedding Model\n",
        "### 2. Convert the Each Chunk of The Split Document to Embedding Vectors\n",
        "### 3. Storing of The Embedding Vectors to Vectorstore\n",
        "### 4. Save the Vectorstore to Your Drive"
      ],
      "metadata": {
        "id": "4ENim_5MOT9O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "embeddings=OpenAIEmbeddings(model=\"text-embedding-3-large\",\n",
        "                            dimensions=3072) #dimensions=256, 1024, 3072\n",
        "embeddings\n",
        "\n",
        "# As the embedding model, we use Openai's latest introduced text-embedding-3-large model.\n",
        "# dimensions of text-embedding-3-large are 256, 1024 and 3072\n",
        "# dimensions of text-embedding-3-small are 512 and 1536\n",
        "# dimension of text-embedding-ada-002 is only 1536\n",
        "# text-embedding-3-large gives the best embedding performance"
      ],
      "metadata": {
        "id": "96nLFF1ja0k_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee36902c-2217-4ebb-f898-325b4db35e9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OpenAIEmbeddings(client=<openai.resources.embeddings.Embeddings object at 0x7ae9efd80e50>, async_client=<openai.resources.embeddings.AsyncEmbeddings object at 0x7ae9efdb1a50>, model='text-embedding-3-large', dimensions=3072, deployment='text-embedding-ada-002', openai_api_version='', openai_api_base=None, openai_api_type='', openai_proxy='', embedding_ctx_length=8191, openai_api_key=SecretStr('**********'), openai_organization=None, allowed_special=None, disallowed_special=None, chunk_size=1000, max_retries=2, request_timeout=None, headers=None, tiktoken_enabled=True, tiktoken_model_name=None, show_progress_bar=False, model_kwargs={}, skip_empty=False, default_headers=None, default_query=None, retry_min_seconds=4, retry_max_seconds=20, http_client=None, http_async_client=None, check_embedding_ctx_length=True)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Vectorstore(index) From Your Drive"
      ],
      "metadata": {
        "id": "y2tMqUthPchD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.vectorstores import Chroma\n",
        "\n",
        "index=Chroma().from_documents(documents=pdf_doc,\n",
        "                              embedding=embeddings,\n",
        "                              persist_directory=\"/content/vectorstore2\") # persist_directory, saves in the directory"
      ],
      "metadata": {
        "id": "5pjKXmO6a3En"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_index=Chroma(persist_directory=\"/content/vectorstore2\",\n",
        "                    embedding_function=embeddings)"
      ],
      "metadata": {
        "id": "AXQwX35ha282"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Retrival the First 5 Chunks That Are Most Similar to The User Query from The Document"
      ],
      "metadata": {
        "id": "pKA0PgNJQOmj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_query(query,k=5):\n",
        "    matching_results=index.similarity_search(query,k=k) #loaded_index if working on the drive\n",
        "    return matching_results\n",
        "\n",
        "# The query we ask is first converted into an embedding vector. Then, using the cosine similarity metric, the similarity scores between this vector\n",
        "# and the embedding vectors in the vector store are calculated and sorted. The top 4 most similar texts are returned\n",
        "# Best k: If split was made on paragraph based than 4 or 5 is ok"
      ],
      "metadata": {
        "id": "KRH-FWEua5Fn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generating an Answer Based on The Similar Chunks"
      ],
      "metadata": {
        "id": "-G8R4V7BROkz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "our_query = \"What is BERT?\"\n",
        "\n",
        "doc_search=retrieve_query(our_query, k=2) # first two most similar texts are returned\n",
        "doc_search"
      ],
      "metadata": {
        "id": "XNDU0jcma7HB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88ef760c-670e-41a6-ba1e-0f328ae71de8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='unlabeled text by jointly conditioning on both\\r\\nleft and right context in all layers. As a re\\x02sult, the pre-trained BERT model can be fine\\x02tuned with just one additional output layer\\r\\nto create state-of-the-art models for a wide\\r\\nrange of tasks, such as question answering and\\r\\nlanguage inference, without substantial task\\x02specific architecture modifications.\\r\\nBERT is conceptually simple and empirically\\r\\npowerful. It obtains new state-of-the-art re\\x02sults on eleven natural language processing\\r\\ntasks, including pushing the GLUE score to\\r\\n80.5% (7.7% point absolute improvement),\\r\\nMultiNLI accuracy to 86.7% (4.6% absolute\\r\\nimprovement), SQuAD v1.1 question answer\\x02ing Test F1 to 93.2 (1.5 point absolute im\\x02provement) and SQuAD v2.0 Test F1 to 83.1\\r\\n(5.1 point absolute improvement).', metadata={'page': 0, 'source': '/content/drive/MyDrive/FINAL PROJJECTS/N19-1423.pdf'}),\n",
              " Document(page_content='4183\\r\\nBERT (Ours)\\r\\nTrm Trm Trm\\r\\nTrm Trm Trm\\r\\n...\\r\\n...\\r\\nTrm Trm Trm\\r\\nTrm Trm Trm\\r\\n...\\r\\n...\\r\\nOpenAI GPT\\r\\nLstm\\r\\nELMo\\r\\nLstm Lstm\\r\\nLstm Lstm Lstm\\r\\nLstm Lstm Lstm\\r\\nLstm Lstm Lstm\\r\\n T1 T2\\r\\n TN\\r\\n...\\r\\n...\\r\\n...\\r\\n...\\r\\n...\\r\\n E1 E2\\r\\n EN\\r\\n...\\r\\n T1 T2 TN\\r\\n...\\r\\n E1 E2\\r\\n E ... N\\r\\n T1 T2\\r\\n TN\\r\\n...\\r\\n E1 E2\\r\\n EN\\r\\n...\\r\\nFigure 3: Differences in pre-training model architectures. BERT uses a bidirectional Transformer. OpenAI GPT\\r\\nuses a left-to-right Transformer. ELMo uses the concatenation of independently trained left-to-right and right-to\\x02left LSTMs to generate features for downstream tasks. Among the three, only BERT representations are jointly\\r\\nconditioned on both left and right context in all layers. In addition to the architecture differences, BERT and', metadata={'page': 12, 'source': '/content/drive/MyDrive/FINAL PROJJECTS/N19-1423.pdf'})]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pipeline For RAG (If you want, you can use the gemini-1.5-pro model)"
      ],
      "metadata": {
        "id": "sy4fmzsWLayT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import OpenAI, ChatOpenAI\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "import textwrap\n",
        "\n",
        "#https://stackoverflow.com/questions/76692869/how-to-add-memory-to-load-qa-chain-or-how-to-implement-conversationalretrievalch"
      ],
      "metadata": {
        "id": "zROT3Ut1fbi3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# stuff means do not split the chunks further, map reduce means you can split further\n",
        "llm=ChatOpenAI(model_name=\"gpt-3.5-turbo-0125\", temperature=0)\n",
        "chain=load_qa_chain(llm, chain_type=\"stuff\")"
      ],
      "metadata": {
        "id": "_aSt7YgIa9jo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain"
      ],
      "metadata": {
        "id": "2PzfTncDa9dO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "348895c7-13a1-4417-b377-c9deb53073c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StuffDocumentsChain(llm_chain=LLMChain(prompt=ChatPromptTemplate(input_variables=['context', 'question'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], template=\"Use the following pieces of context to answer the user's question. \\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\\n----------------\\n{context}\")), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], template='{question}'))]), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7ae9ed140fa0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7ae9e3175fc0>, model_name='gpt-3.5-turbo-0125', temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy='')), document_variable_name='context')"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(chain.llm_chain.prompt.messages[0].prompt.template)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uctQ2NTPebXk",
        "outputId": "302cd51b-b39b-404a-e2f4-264d1c204189"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Use the following pieces of context to answer the user's question. \n",
            "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
            "----------------\n",
            "{context}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "our_query = \"What is BERT?\"\n",
        "\n",
        "doc_search=retrieve_query(our_query)\n",
        "doc_search"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nsVZA_hpek4v",
        "outputId": "8962a4ae-0fdb-4426-9964-9b5c08e86862"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='unlabeled text by jointly conditioning on both\\r\\nleft and right context in all layers. As a re\\x02sult, the pre-trained BERT model can be fine\\x02tuned with just one additional output layer\\r\\nto create state-of-the-art models for a wide\\r\\nrange of tasks, such as question answering and\\r\\nlanguage inference, without substantial task\\x02specific architecture modifications.\\r\\nBERT is conceptually simple and empirically\\r\\npowerful. It obtains new state-of-the-art re\\x02sults on eleven natural language processing\\r\\ntasks, including pushing the GLUE score to\\r\\n80.5% (7.7% point absolute improvement),\\r\\nMultiNLI accuracy to 86.7% (4.6% absolute\\r\\nimprovement), SQuAD v1.1 question answer\\x02ing Test F1 to 93.2 (1.5 point absolute im\\x02provement) and SQuAD v2.0 Test F1 to 83.1\\r\\n(5.1 point absolute improvement).', metadata={'page': 0, 'source': '/content/drive/MyDrive/FINAL PROJJECTS/N19-1423.pdf'}),\n",
              " Document(page_content='4183\\r\\nBERT (Ours)\\r\\nTrm Trm Trm\\r\\nTrm Trm Trm\\r\\n...\\r\\n...\\r\\nTrm Trm Trm\\r\\nTrm Trm Trm\\r\\n...\\r\\n...\\r\\nOpenAI GPT\\r\\nLstm\\r\\nELMo\\r\\nLstm Lstm\\r\\nLstm Lstm Lstm\\r\\nLstm Lstm Lstm\\r\\nLstm Lstm Lstm\\r\\n T1 T2\\r\\n TN\\r\\n...\\r\\n...\\r\\n...\\r\\n...\\r\\n...\\r\\n E1 E2\\r\\n EN\\r\\n...\\r\\n T1 T2 TN\\r\\n...\\r\\n E1 E2\\r\\n E ... N\\r\\n T1 T2\\r\\n TN\\r\\n...\\r\\n E1 E2\\r\\n EN\\r\\n...\\r\\nFigure 3: Differences in pre-training model architectures. BERT uses a bidirectional Transformer. OpenAI GPT\\r\\nuses a left-to-right Transformer. ELMo uses the concatenation of independently trained left-to-right and right-to\\x02left LSTMs to generate features for downstream tasks. Among the three, only BERT representations are jointly\\r\\nconditioned on both left and right context in all layers. In addition to the architecture differences, BERT and', metadata={'page': 12, 'source': '/content/drive/MyDrive/FINAL PROJJECTS/N19-1423.pdf'}),\n",
              " Document(page_content='4173\\r\\nBERT BERT\\r\\nE[CLS] E1\\r\\n E[SEP] ... EN\\r\\nE1’ ... EM’\\r\\nC T1 T[SEP] ... TN\\r\\nT1’ ... TM’\\r\\n[CLS] Tok 1 [SEP] ... Tok N Tok 1 ... TokM\\r\\nQuestion Paragraph\\r\\nStart/End Span\\r\\nBERT\\r\\nE[CLS] E1\\r\\n E[SEP] ... EN\\r\\nE1’ ... EM’\\r\\nC T1 T[SEP] ... TN\\r\\nT1’ ... TM’\\r\\n[CLS] Tok 1 [SEP] ... Tok N Tok 1 ... TokM\\r\\nMasked Sentence A Masked Sentence B\\r\\nPre-training Fine-Tuning\\r\\nNSP Mask LM Mask LM\\r\\nUnlabeled Sentence A and B Pair \\r\\nSQuAD\\r\\nQuestion Answer Pair\\r\\nMNLI NER\\r\\nFigure 1: Overall pre-training and fine-tuning procedures for BERT. Apart from output layers, the same architec\\x02tures are used in both pre-training and fine-tuning. The same pre-trained model parameters are used to initialize\\r\\nmodels for different down-stream tasks. During fine-tuning, all parameters are fine-tuned. [CLS] is a special', metadata={'page': 2, 'source': '/content/drive/MyDrive/FINAL PROJJECTS/N19-1423.pdf'}),\n",
              " Document(page_content='In this paper, we improve the fine-tuning based\\r\\napproaches by proposing BERT: Bidirectional\\r\\nEncoder Representations from Transformers.\\r\\nBERT alleviates the previously mentioned unidi\\x02rectionality constraint by using a “masked lan\\x02guage model” (MLM) pre-training objective, in\\x02spired by the Cloze task (Taylor, 1953). The\\r\\nmasked language model randomly masks some of\\r\\nthe tokens from the input, and the objective is to\\r\\npredict the original vocabulary id of the masked', metadata={'page': 0, 'source': '/content/drive/MyDrive/FINAL PROJJECTS/N19-1423.pdf'}),\n",
              " Document(page_content='expressive pre-trained representations even when\\r\\ndownstream task data is very small.\\r\\n5.3 Feature-based Approach with BERT\\r\\nAll of the BERT results presented so far have used\\r\\nthe fine-tuning approach, where a simple classifi\\x02cation layer is added to the pre-trained model, and\\r\\nall parameters are jointly fine-tuned on a down\\x02stream task. However, the feature-based approach,\\r\\nwhere fixed features are extracted from the pre\\x02trained model, has certain advantages. First, not\\r\\nall tasks can be easily represented by a Trans\\x02former encoder architecture, and therefore require\\r\\na task-specific model architecture to be added.\\r\\nSecond, there are major computational benefits\\r\\nto pre-compute an expensive representation of the\\r\\ntraining data once and then run many experiments', metadata={'page': 8, 'source': '/content/drive/MyDrive/FINAL PROJJECTS/N19-1423.pdf'})]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output= chain.invoke(input={\"input_documents\":doc_search, \"question\":our_query})[\"output_text\"]\n",
        "output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "nAPFR5a-ezGP",
        "outputId": "4e87391f-7582-423f-f7a0-3149a9ba668a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'BERT stands for Bidirectional Encoder Representations from Transformers. It is a pre-trained model that can be fine-tuned for various natural language processing tasks such as question answering and language inference. BERT uses a bidirectional Transformer architecture and is trained using a \"masked language model\" pre-training objective to generate context-aware representations of words. It has achieved state-of-the-art results on multiple NLP tasks.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wrapped_text = textwrap.fill(output, width=100)\n",
        "print(wrapped_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzbFcjC4e310",
        "outputId": "1b951d72-1e89-421d-b040-0ecebf06a3ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BERT stands for Bidirectional Encoder Representations from Transformers. It is a pre-trained model\n",
            "that can be fine-tuned for various natural language processing tasks such as question answering and\n",
            "language inference. BERT uses a bidirectional Transformer architecture and is trained using a\n",
            "\"masked language model\" pre-training objective to generate context-aware representations of words.\n",
            "It has achieved state-of-the-art results on multiple NLP tasks.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_answers(query):\n",
        "    doc_search=retrieve_query(query)\n",
        "    response=chain.invoke(input={\"input_documents\":doc_search, \"question\":query})[\"output_text\"]\n",
        "    wrapped_text = textwrap.fill(response, width=100)\n",
        "    return wrapped_text"
      ],
      "metadata": {
        "id": "Y9ycM0uYfRjM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "our_query = \"What is BERT?\"\n",
        "answer = get_answers(our_query)\n",
        "print(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FPF0QXMSfs1W",
        "outputId": "87a5fe26-3a88-4650-c2cb-c198c9816e78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BERT stands for Bidirectional Encoder Representations from Transformers. It is a pre-trained model\n",
            "that uses a bidirectional Transformer architecture to generate contextualized word embeddings. BERT\n",
            "can be fine-tuned with just one additional output layer to achieve state-of-the-art performance on\n",
            "various natural language processing tasks such as question answering and language inference. It\n",
            "overcomes the unidirectionality constraint by using a \"masked language model\" pre-training\n",
            "objective, allowing it to capture contextual information from both left and right contexts in all\n",
            "layers.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "our_query = \"What is the capital of USA?\"\n",
        "answer = get_answers(our_query)\n",
        "print(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9rYTm7btf0lF",
        "outputId": "68081baf-690c-4121-986b-97136dd29bb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I don't know.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Project 2: Generating PDF Document Summaries\n",
        "\n",
        "In this project, you will explore various methods for creating summaries from the provided PDF document. You will experiment with different chaining functions offered by the Langchain library to achieve this.\n",
        "\n",
        "### **Project Steps:**\n",
        "- **1.PDF Document Upload and Chunking:** As in the first project, upload the PDF document and divide it into smaller chunks. Consider splitting it by half-page or page.\n",
        "\n",
        "- **2.Summarization Techniques:**\n",
        "\n",
        "  - **Summary of the First 5 Pages (Stuff Chain):** Utilize the load_summarize_chain function with the parameter chain_type=\"stuff\" to generate a concise summary of the first 5 pages of the PDF document.\n",
        "\n",
        "  - **Short Summary of the Entire Document (Map Reduce Chain):** Employ chain_type=\"map_reduce\" and refine parameters to create a brief summary of the entire document. This method generates individual summaries for each chunk and then combines them into a final summary.\n",
        "\n",
        "  - **Detailed Summary with Bullet Points (Map Reduce Chain):** Use chain_type=\"map_reduce\" to generate a detailed summary with at least 1000 tokens. Provide the LLM with the prompt \"Summarize with 1000 tokens\" and set the max_token parameter to a value greater than 1000. Add a title to the summary and present key points using bullet points.\n",
        "\n",
        "### Important Notes:\n",
        "\n",
        "- Models like GPT-4 and Gemini Pro models might excel in generating summaries based on token count. Consider prioritizing these models.\n",
        "\n",
        "- For comprehensive information on Langchain and LLMs, refer to their respective documentation.\n",
        "Best of luck!"
      ],
      "metadata": {
        "id": "H9GmKlL2NRff"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install Libraries"
      ],
      "metadata": {
        "id": "WhjLe0IqRnl4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain"
      ],
      "metadata": {
        "id": "ZXdV8CcqbFrW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60e9f269-7ba7-4344-ffeb-6e48eedfb49d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.1.20)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.30)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.6.6)\n",
            "Requirement already satisfied: langchain-community<0.1,>=0.0.38 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.38)\n",
            "Requirement already satisfied: langchain-core<0.2.0,>=0.1.52 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.52)\n",
            "Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.1)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.57)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.7.1)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.3.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.21.2)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.52->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.52->langchain) (23.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.3)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.18.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2.0,>=0.1.52->langchain) (2.4)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U pypdfium2"
      ],
      "metadata": {
        "id": "rcFsXQwCbFkm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65c31181-8026-46df-fcd8-fd476369c1b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pypdfium2 in /usr/local/lib/python3.10/dist-packages (4.30.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading PDF Document"
      ],
      "metadata": {
        "id": "yqImlx_IRqQS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PyPDFium2Loader\n",
        "\n",
        "def read_doc(directory):\n",
        "    file_loader=PyPDFium2Loader(directory)\n",
        "    pdf_documents=file_loader.load()\n",
        "    return pdf_documents"
      ],
      "metadata": {
        "id": "CCkT3msfbH_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pdf=read_doc('/content/drive/MyDrive/FINAL PROJJECTS/N19-1423.pdf')\n",
        "len(pdf)"
      ],
      "metadata": {
        "id": "5a_FpBOcbHzP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1887d03-4003-4655-8c89-1cb2c7cd5c71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pypdfium2/_helpers/textpage.py:80: UserWarning: get_text_range() call with default params will be implicitly redirected to get_text_bounded()\n",
            "  warnings.warn(\"get_text_range() call with default params will be implicitly redirected to get_text_bounded()\")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pdf[3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46sSyOWPl5WF",
        "outputId": "1d77a31c-dbf4-43ad-82c6-3c2adce53e21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content='4174\\r\\nInput/Output Representations To make BERT\\r\\nhandle a variety of down-stream tasks, our input\\r\\nrepresentation is able to unambiguously represent\\r\\nboth a single sentence and a pair of sentences\\r\\n(e.g., h Question, Answeri) in one token sequence.\\r\\nThroughout this work, a “sentence” can be an arbi\\x02trary span of contiguous text, rather than an actual\\r\\nlinguistic sentence. A “sequence” refers to the in\\x02put token sequence to BERT, which may be a sin\\x02gle sentence or two sentences packed together.\\r\\nWe use WordPiece embeddings (Wu et al.,\\r\\n2016) with a 30,000 token vocabulary. The first\\r\\ntoken of every sequence is always a special clas\\x02sification token ([CLS]). The final hidden state\\r\\ncorresponding to this token is used as the ag\\x02gregate sequence representation for classification\\r\\ntasks. Sentence pairs are packed together into a\\r\\nsingle sequence. We differentiate the sentences in\\r\\ntwo ways. First, we separate them with a special\\r\\ntoken ([SEP]). Second, we add a learned embed\\x02ding to every token indicating whether it belongs\\r\\nto sentence A or sentence B. As shown in Figure 1,\\r\\nwe denote input embedding as E, the final hidden\\r\\nvector of the special [CLS] token as C ∈ R\\r\\nH,\\r\\nand the final hidden vector for the i\\r\\nth input token\\r\\nas Ti ∈ R\\r\\nH.\\r\\nFor a given token, its input representation is\\r\\nconstructed by summing the corresponding token,\\r\\nsegment, and position embeddings. A visualiza\\x02tion of this construction can be seen in Figure 2.\\r\\n3.1 Pre-training BERT\\r\\nUnlike Peters et al. (2018a) and Radford et al.\\r\\n(2018), we do not use traditional left-to-right or\\r\\nright-to-left language models to pre-train BERT.\\r\\nInstead, we pre-train BERT using two unsuper\\x02vised tasks, described in this section. This step\\r\\nis presented in the left part of Figure 1.\\r\\nTask #1: Masked LM Intuitively, it is reason\\x02able to believe that a deep bidirectional model is\\r\\nstrictly more powerful than either a left-to-right\\r\\nmodel or the shallow concatenation of a left-to\\x02right and a right-to-left model. Unfortunately,\\r\\nstandard conditional language models can only be\\r\\ntrained left-to-right or right-to-left, since bidirec\\x02tional conditioning would allow each word to in\\x02directly “see itself”, and the model could trivially\\r\\npredict the target word in a multi-layered context.\\r\\nformer is often referred to as a “Transformer encoder” while\\r\\nthe left-context-only version is referred to as a “Transformer\\r\\ndecoder” since it can be used for text generation.\\r\\nIn order to train a deep bidirectional representa\\x02tion, we simply mask some percentage of the input\\r\\ntokens at random, and then predict those masked\\r\\ntokens. We refer to this procedure as a “masked\\r\\nLM” (MLM), although it is often referred to as a\\r\\nCloze task in the literature (Taylor, 1953). In this\\r\\ncase, the final hidden vectors corresponding to the\\r\\nmask tokens are fed into an output softmax over\\r\\nthe vocabulary, as in a standard LM. In all of our\\r\\nexperiments, we mask 15% of all WordPiece to\\x02kens in each sequence at random. In contrast to\\r\\ndenoising auto-encoders (Vincent et al., 2008), we\\r\\nonly predict the masked words rather than recon\\x02structing the entire input.\\r\\nAlthough this allows us to obtain a bidirec\\x02tional pre-trained model, a downside is that we\\r\\nare creating a mismatch between pre-training and\\r\\nfine-tuning, since the [MASK] token does not ap\\x02pear during fine-tuning. To mitigate this, we do\\r\\nnot always replace “masked” words with the ac\\x02tual [MASK] token. The training data generator\\r\\nchooses 15% of the token positions at random for\\r\\nprediction. If the i-th token is chosen, we replace\\r\\nthe i-th token with (1) the [MASK] token 80% of\\r\\nthe time (2) a random token 10% of the time (3)\\r\\nthe unchanged i-th token 10% of the time. Then,\\r\\nTi will be used to predict the original token with\\r\\ncross entropy loss. We compare variations of this\\r\\nprocedure in Appendix C.2.\\r\\nTask #2: Next Sentence Prediction (NSP)\\r\\nMany important downstream tasks such as Ques\\x02tion Answering (QA) and Natural Language Infer\\x02ence (NLI) are based on understanding the rela\\x02tionship between two sentences, which is not di\\x02rectly captured by language modeling. In order\\r\\nto train a model that understands sentence rela\\x02tionships, we pre-train for a binarized next sen\\x02tence prediction task that can be trivially gener\\x02ated from any monolingual corpus. Specifically,\\r\\nwhen choosing the sentences A and B for each pre\\x02training example, 50% of the time B is the actual\\r\\nnext sentence that follows A (labeled as IsNext),\\r\\nand 50% of the time it is a random sentence from\\r\\nthe corpus (labeled as NotNext). As we show\\r\\nin Figure 1, C is used for next sentence predic\\x02tion (NSP).5 Despite its simplicity, we demon\\x02strate in Section 5.1 that pre-training towards this\\r\\ntask is very beneficial to both QA and NLI. 6\\r\\n5The final model achieves 97%-98% accuracy on NSP.\\r\\n6The vector C is not a meaningful sentence representation\\r\\nwithout fine-tuning, since it was trained with NSP.\\n', metadata={'source': '/content/drive/MyDrive/FINAL PROJJECTS/N19-1423.pdf', 'page': 3})"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Summarizing the First 5 Pages of The Document With Chain_Type of The 'stuff'"
      ],
      "metadata": {
        "id": "LuyT0IoWR4n8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.chains.summarize import load_summarize_chain\n",
        "\n",
        "llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-0125',max_tokens=1024)"
      ],
      "metadata": {
        "id": "O3yAnW3PbKIX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = load_summarize_chain(\n",
        "    llm,\n",
        "    chain_type='stuff',\n",
        "    verbose=False\n",
        ")\n",
        "output_summary = chain.invoke(pdf[0:5])['output_text']"
      ],
      "metadata": {
        "id": "8wopgGPibKA3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_summary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "G5XQxgvemehF",
        "outputId": "81abf8ad-c4a0-4343-f4ca-ecc3488f69fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The paper introduces BERT, a language representation model that pre-trains deep bidirectional representations from unlabeled text by conditioning on both left and right context. BERT can be fine-tuned with minimal modifications to achieve state-of-the-art results on various natural language processing tasks. It uses a masked language model and next sentence prediction task for pre-training, and fine-tuning is straightforward due to the self-attention mechanism in the Transformer. BERT outperforms previous models on 11 NLP tasks, including question answering and language inference. The paper provides detailed information on the model architecture, pre-training, fine-tuning, and experimental results on the GLUE benchmark.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n",
        "\n",
        "wrapped_text=textwrap.fill(output_summary, width=100)\n",
        "print(wrapped_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xw3rCQbZmjVJ",
        "outputId": "98bc2399-0cf7-49a6-d8e2-8c116667ed04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The paper introduces BERT, a language representation model that pre-trains deep bidirectional\n",
            "representations from unlabeled text by conditioning on both left and right context. BERT can be\n",
            "fine-tuned with minimal modifications to achieve state-of-the-art results on various natural\n",
            "language processing tasks. It uses a masked language model and next sentence prediction task for\n",
            "pre-training, and fine-tuning is straightforward due to the self-attention mechanism in the\n",
            "Transformer. BERT outperforms previous models on 11 NLP tasks, including question answering and\n",
            "language inference. The paper provides detailed information on the model architecture, pre-training,\n",
            "fine-tuning, and experimental results on the GLUE benchmark.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Document Splitter"
      ],
      "metadata": {
        "id": "JvrLsoivTulb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000, chunk_overlap=0)\n",
        "chunks = text_splitter.split_documents(pdf)"
      ],
      "metadata": {
        "id": "lLhZfxtOweWV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(chunks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x5OaM9vSwksK",
        "outputId": "49bf185b-2c30-47f9-d110-0e3a4412a9e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Make A Brief Summary of The Entire Document With Chain_Types of \"map_reduce\" and \"refine\""
      ],
      "metadata": {
        "id": "3zlVe2iISX0Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chain = load_summarize_chain(llm,\n",
        "                             chain_type=\"map_reduce\")\n",
        "\n",
        "\n",
        "output_summary = chain.invoke(chunks)[\"output_text\"]\n",
        "wrapped_text = textwrap.fill(output_summary, width=100)\n",
        "print(wrapped_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M2TkYpBiv_kZ",
        "outputId": "4527219e-263d-45aa-85fc-c4cc41744433"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The paper introduces BERT, a language representation model that pretrains deep bidirectional\n",
            "representations from unlabeled text, achieving state-of-the-art results on various NLP tasks through\n",
            "fine-tuning with just one additional output layer. BERT addresses limitations of unidirectional\n",
            "models, uses masked language model pre-training, and demonstrates effectiveness in tasks like\n",
            "Question Answering and Natural Language Inference. It outperforms other systems on the GLUE\n",
            "benchmark and SQuAD tasks, with larger models showing improved accuracy. Transfer learning with BERT\n",
            "benefits a wide range of NLP tasks, and the study compares BERT with other models like ELMo and\n",
            "OpenAI GPT. Fine-tuning BERT on different tasks improves performance, and the study explores the\n",
            "impact of pre-training steps and masking strategies on model accuracy.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain = load_summarize_chain(llm,\n",
        "                             chain_type=\"refine\")\n",
        "\n",
        "output_summary = chain.invoke(chunks)[\"output_text\"]"
      ],
      "metadata": {
        "id": "QqppiBIwGUqV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wrapped_text = textwrap.fill(output_summary, width=100)\n",
        "print(wrapped_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qTntLSa7Ger7",
        "outputId": "b1fc8625-7d67-422f-dd92-bf68d11daadf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The existing summary provides a detailed comparison of pre-training model architectures including\n",
            "BERT, OpenAI GPT, and ELMo. It also explains the pre-training and fine-tuning procedures for BERT,\n",
            "highlighting the use of a bidirectional Transformer and the training setup on Cloud TPUs.\n",
            "Additionally, it mentions the optimization of training with different sequence lengths to balance\n",
            "efficiency and effectiveness. The summary concludes with a note on fine-tuning hyperparameters for\n",
            "specific tasks. The new context includes a comparison of hyperparameters such as learning rates and\n",
            "number of epochs, as well as a comparison of BERT with OpenAI GPT in terms of training data,\n",
            "architecture, and fine-tuning approaches. It also discusses the illustration of fine-tuning BERT on\n",
            "different tasks and provides detailed descriptions of the GLUE benchmark experiments. The additional\n",
            "ablation studies in the new context explore the effect of the number of training steps and different\n",
            "masking procedures on the performance of BERT in tasks like MNLI and NER.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate A Detailed Summary of The Entire Document With At Least 1000 Tokens. Also, Add A Title To The Summary And Present Key Points Using Bullet Points With Chain_Type of \"map_reduce\"."
      ],
      "metadata": {
        "id": "9zZxse-ZUV3S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chain = load_summarize_chain(\n",
        "    llm=llm,\n",
        "    chain_type='map_reduce'\n",
        ")\n",
        "chain"
      ],
      "metadata": {
        "id": "lpuiEedJbQME",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "947e0538-9a7a-49e1-cfb3-32e908cc9977"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MapReduceDocumentsChain(llm_chain=LLMChain(prompt=PromptTemplate(input_variables=['text'], template='Write a concise summary of the following:\\n\\n\\n\"{text}\"\\n\\n\\nCONCISE SUMMARY:'), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7ae9e30ce5f0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7ae9e30cef50>, model_name='gpt-3.5-turbo-0125', temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy='', max_tokens=1024)), reduce_documents_chain=ReduceDocumentsChain(combine_documents_chain=StuffDocumentsChain(llm_chain=LLMChain(prompt=PromptTemplate(input_variables=['text'], template='Write a concise summary of the following:\\n\\n\\n\"{text}\"\\n\\n\\nCONCISE SUMMARY:'), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7ae9e30ce5f0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7ae9e30cef50>, model_name='gpt-3.5-turbo-0125', temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy='', max_tokens=1024)), document_variable_name='text')), document_variable_name='text')"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt for combined summaries\n",
        "chain.reduce_documents_chain.combine_documents_chain.llm_chain.prompt.template"
      ],
      "metadata": {
        "id": "OdFF9bOAbQEo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "e4a93f09-2ea6-4ed8-953d-ba61f91de558"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Write a concise summary of the following:\\n\\n\\n\"{text}\"\\n\\n\\nCONCISE SUMMARY:'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt for every chunk\n",
        "from langchain import PromptTemplate\n",
        "\n",
        "chunks_prompt=\"\"\"\n",
        "Please summarize the below text:\n",
        "text:'{text}'\n",
        "summary:\n",
        "\"\"\"\n",
        "map_prompt_template=PromptTemplate(input_variables=['text'],\n",
        "                                   template=chunks_prompt)"
      ],
      "metadata": {
        "id": "FG6QxTZQzWUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt for combined summaries\n",
        "from langchain import PromptTemplate\n",
        "final_combine_prompt='''\n",
        "Provide a final summary of the entire text with at least 1000 words.\n",
        "Add a Generic  Title,\n",
        "Start the precise summary with an introduction and provide the\n",
        "summary in bullet points for the text.\n",
        "text: '{text}'\n",
        "summary:\n",
        "'''\n",
        "final_combine_prompt_template=PromptTemplate(input_variables=['text'],\n",
        "                                             template=final_combine_prompt)"
      ],
      "metadata": {
        "id": "XDtb74XWxepe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = load_summarize_chain(\n",
        "                            llm=llm,\n",
        "                            chain_type='map_reduce',\n",
        "                            map_prompt=map_prompt_template,\n",
        "                            combine_prompt=final_combine_prompt_template\n",
        ")\n",
        "chain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gqoT0ocgyxJR",
        "outputId": "c38ed965-ff93-4393-8b5b-70d8a0201ddf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MapReduceDocumentsChain(llm_chain=LLMChain(prompt=PromptTemplate(input_variables=['text'], template=\"\\nPlease summarize the below text:\\ntext:'{text}'\\nsummary:\\n\"), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7ae9e30ce5f0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7ae9e30cef50>, model_name='gpt-3.5-turbo-0125', temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy='', max_tokens=1024)), reduce_documents_chain=ReduceDocumentsChain(combine_documents_chain=StuffDocumentsChain(llm_chain=LLMChain(prompt=PromptTemplate(input_variables=['text'], template=\"\\nProvide a final summary of the entire text with at least 1000 words.\\nAdd a Generic  Title,\\nStart the precise summary with an introduction and provide the\\nsummary in bullet points for the text.\\ntext: '{text}'\\nsummary:\\n\"), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7ae9e30ce5f0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7ae9e30cef50>, model_name='gpt-3.5-turbo-0125', temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy='', max_tokens=1024)), document_variable_name='text')), document_variable_name='text')"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_summary = chain.invoke(chunks)[\"output_text\"]\n",
        "wrapped_text = textwrap.fill(output_summary, replace_whitespace=False, width=200 )\n",
        "print(wrapped_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Patq7R-7zfFS",
        "outputId": "d1ab81a1-f010-45ef-cf30-feb654d8e68a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Title: A Comprehensive Overview of BERT: Bidirectional Encoder Representations from Transformers\n",
            "\n",
            "Introduction:\n",
            "The text introduces BERT, a language representation model designed for bidirectional\n",
            "pretraining from unlabeled text. BERT has shown state-of-the-art results on various natural language processing tasks by incorporating bidirectional context.\n",
            "\n",
            "Summary:\n",
            "- BERT, or Bidirectional Encoder\n",
            "Representations from Transformers, is a language representation model that pretrains deep bidirectional representations from unlabeled text.\n",
            "- The model can be fine-tuned with just one additional\n",
            "output layer to achieve state-of-the-art results on multiple NLP tasks.\n",
            "- BERT uses a masked language model pre-training objective to incorporate bidirectional context, improving upon existing\n",
            "techniques.\n",
            "- The importance of bidirectional pre-training for language representations is highlighted, reducing the need for task-specific architectures.\n",
            "- BERT advances the state of the art for\n",
            "eleven NLP tasks and outperforms other models on the GLUE benchmark.\n",
            "- The pre-training and fine-tuning procedures for BERT are discussed, along with the model architecture based on a multi-layer\n",
            "bidirectional Transformer encoder.\n",
            "- Input/output representations in BERT include WordPiece embeddings, special tokens, and segment embeddings for handling downstream tasks.\n",
            "- BERT is pre-trained on a\n",
            "large corpus of text data and fine-tuned for various NLP tasks, showing high performance and efficiency in training.\n",
            "- Results on the GLUE benchmark tasks demonstrate the superiority of BERTBASE and\n",
            "BERTLARGE models over other systems.\n",
            "- Ablation studies on BERTBASE show the impact of pre-training tasks and model size on fine-tuning task accuracy, emphasizing the importance of bidirectional\n",
            "models and larger sizes.\n",
            "- Increasing the size of pre-trained bi-directional language models, such as BERT, improves performance on downstream tasks like Named Entity Recognition.\n",
            "- Transfer learning\n",
            "with language models like BERT is crucial for enhancing language understanding systems, especially for low-resource tasks.\n",
            "- The text provides references related to various topics in NLP, including\n",
            "sequence labeling, language modeling, and semantic textual similarity.\n",
            "- Additional research papers related to natural language understanding, neural networks, and language comprehension are\n",
            "discussed, along with an appendix detailing BERT implementation and experiments.\n",
            "- Comparison with other pre-training model architectures like OpenAI GPT and ELMo is made, highlighting the strengths\n",
            "of BERT in bidirectional context incorporation.\n",
            "- Fine-tuning BERT on different tasks such as sentiment analysis and natural language inference shows its versatility and potential for multitask\n",
            "learning.\n",
            "- Ablation studies on BERT reveal the impact of training steps and masking procedures on model accuracy, emphasizing the robustness of fine-tuning.\n",
            "\n",
            "Conclusion:\n",
            "The text provides a\n",
            "comprehensive overview of BERT, showcasing its effectiveness in pre-training bidirectional language representations and fine-tuning for various NLP tasks. BERT's advancements in language understanding\n",
            "systems, high performance on benchmarks, and robustness to different training strategies make it a valuable model in the field of natural language processing.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "END OF THE PROJECT"
      ],
      "metadata": {
        "id": "3TZTcV7La8gI"
      }
    }
  ]
}